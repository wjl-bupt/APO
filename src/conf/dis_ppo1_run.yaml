# ===========================
# ‚ö°Ô∏è Experiment Configuration
# ===========================
experiment:
  torch_deterministic: true  # Whether to set torch.backends.cudnn.deterministic to True
  cuda: true  # Whether to use CUDA by default
  track: true  # Whether to track the experiment using Weights and Biases
  wandb_project_name: "cleanRL-atari-debugger"  # The project name for WandB
  capture_video: false  # Whether to capture videos of agent performance
  
# ===========================
# ‚ö°Ô∏è Algorithm Specific Arguments
# ===========================
algorithm:
  total_timesteps: 10000000  # The total number of timesteps for the experiment
  learning_rate: !!float 2.5e-4  # The learning rate for the optimizer
  num_envs: 8  # The number of parallel game environments
  num_steps: 128  # The number of steps per policy rollout
  anneal_lr: true  # Whether to toggle learning rate annealing
  gamma: !!float 0.99  # The discount factor gamma
  gae_lambda: !!float 0.95  # The lambda for general advantage estimation
  num_minibatches: 4  # The number of mini-batches
  update_epochs: 4  # The number of epochs for policy updates
  norm_adv: true  # Whether to normalize advantages
  clip_coef: !!float 0.1  # The clipping coefficient for the surrogate objective
  clip_coef_2: !!float 0.1  # The clipping coefficient for the ratio2
  clip_vloss: true  # Whether to use a clipped loss for the value function
  ent_coef: !!float 0.01  # The entropy coefficient
  vf_coef: !!float 0.5  # The value function coefficient
  max_grad_norm: !!float 0.5  # The maximum norm for gradient clipping
  target_kl: !!float 0.01  # The target KL divergence threshold
  penalty_coef: !!float 0.2
  decay_delta: !!float 1.0

# ===========================
# üïπÔ∏è Runtime Configuration
# ===========================
  noise_exp: False
  sample_action_num: 1
